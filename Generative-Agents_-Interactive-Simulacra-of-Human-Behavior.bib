@article{5278a8eb2ba2429d4029745caf4e661080073c81,
title = {Generative Agents: Interactive Simulacra of Human Behavior},
year = {2023},
url = {https://www.semanticscholar.org/paper/5278a8eb2ba2429d4029745caf4e661080073c81},
abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
author = {J. Park and Joseph C. O'Brien and Carrie J. Cai and M. Morris and Percy Liang and Michael S. Bernstein},
journal = {ArXiv},
volume = {abs/2304.03442},
pages = {null},
doi = {10.48550/arXiv.2304.03442},
arxivid = {2304.03442},
}

@article{1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d,
title = {Visual Instruction Tuning},
year = {2023},
url = {https://www.semanticscholar.org/paper/1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d},
abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
author = {Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
journal = {ArXiv},
volume = {abs/2304.08485},
pages = {null},
doi = {10.48550/arXiv.2304.08485},
arxivid = {2304.08485},
}

@article{0d42221038c05cee8443c5b5af838505ee137dc3,
title = {ART: Automatic multi-step reasoning and tool-use for large language models},
year = {2023},
url = {https://www.semanticscholar.org/paper/0d42221038c05cee8443c5b5af838505ee137dc3},
abstract = {Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.},
author = {Bhargavi Paranjape and Scott M. Lundberg and Sameer Singh and Hanna Hajishirzi and Luke Zettlemoyer and Marco Tulio Ribeiro},
journal = {ArXiv},
volume = {abs/2303.09014},
pages = {null},
doi = {10.48550/arXiv.2303.09014},
arxivid = {2303.09014},
}

@article{38fe8f324d2162e63a967a9ac6648974fc4c66f3,
title = {PaLM-E: An Embodied Multimodal Language Model},
year = {2023},
url = {https://www.semanticscholar.org/paper/38fe8f324d2162e63a967a9ac6648974fc4c66f3},
abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
author = {Danny Driess and F. Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Q. Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and P. Sermanet and Daniel Duckworth and S. Levine and Vincent Vanhoucke and Karol Hausman and M. Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Peter R. Florence},
journal = {ArXiv},
volume = {abs/2303.03378},
pages = {null},
doi = {10.48550/arXiv.2303.03378},
arxivid = {2303.03378},
}

@article{f3cf71c51b882fe3111d71c4bf104297d38197f8,
title = {Inner Monologue: Embodied Reasoning through Planning with Language Models},
year = {2022},
url = {https://www.semanticscholar.org/paper/f3cf71c51b882fe3111d71c4bf104297d38197f8},
abstract = {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
author = {Wenlong Huang and F. Xia and Ted Xiao and Harris Chan and Jacky Liang and Peter R. Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and P. Sermanet and Noah Brown and Tomas Jackson and Linda Luu and S. Levine and Karol Hausman and Brian Ichter},
doi = {10.48550/arXiv.2207.05608},
arxivid = {2207.05608},
}

@article{32ac52069e562d4f900afee70bdca63f53461481,
title = {QLoRA: Efficient Finetuning of Quantized LLMs},
year = {2023},
url = {https://www.semanticscholar.org/paper/32ac52069e562d4f900afee70bdca63f53461481},
abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
author = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
journal = {ArXiv},
volume = {abs/2305.14314},
pages = {null},
doi = {10.48550/arXiv.2305.14314},
arxivid = {2305.14314},
}

@article{49b499598a8864eee55ab264fc16a5bf8d2f87ef,
title = {Social Simulacra: Creating Populated Prototypes for Social Computing Systems},
year = {2022},
url = {https://www.semanticscholar.org/paper/49b499598a8864eee55ab264fc16a5bf8d2f87ef},
abstract = {Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer’s description of a community’s design—goal, rules, and member personas—and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of “what if?” scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models’ training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.},
author = {J. Park and Lindsay Popowski and Carrie J. Cai and M. Morris and Percy Liang and Michael S. Bernstein},
journal = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
volume = {null},
pages = {null},
doi = {10.1145/3526113.3545616},
arxivid = {2208.04024},
}

@article{ba704774f194938b04b1e2be40b1d111a4ca08e1,
title = {CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation},
year = {2023},
url = {https://www.semanticscholar.org/paper/ba704774f194938b04b1e2be40b1d111a4ca08e1},
abstract = {Large Language Models (LLMs) have demonstrated significant progress in utilizing external APIs as tools for various tasks. However, their tool-using ability is limited by the availability of suitable APIs and the instability of implicit reasoning, particularly when simultaneously engaging in reasoning about plans and actual calculations. To address these limitations, we propose CREATOR, a novel framework that empowers LLMs to create their own tools through documentation and code realization. CREATOR disentangles the LLM's ability into two distinct phases: abstract tool creation and concrete decision execution, which results in improved LLM performance. We evaluate CREATOR on two established benchmarks: MATH, which consists of challenging math competition problems, and TabMWP, which includes diverse tabular contents for problem-solving. Remarkably, CREATOR significantly outperforms existing chain-of-thought (CoT), program-of-thought (PoT), and tool-using baselines on these two benchmarks. Additionally, we present a new dataset, Creation Challenge, comprising 2K diverse questions, to highlight the necessity and benefits of LLMs' tool creation ability in effectively addressing these problems. Furthermore, our research reveals that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to flexibly tackle diverse situations. Our study represents a promising avenue for maximizing the potential of LLMs and advancing toward truly intelligent and adaptable AI systems.},
author = {Cheng Qian and Chi Han and Y. Fung and Yujia Qin and Zhiyuan Liu and Heng Ji},
journal = {ArXiv},
volume = {abs/2305.14318},
pages = {null},
doi = {10.48550/arXiv.2305.14318},
arxivid = {2305.14318},
}

@article{473eb062612a17c965eaa62136322f0dec6b1f8e,
title = {Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow},
year = {2023},
url = {https://www.semanticscholar.org/paper/473eb062612a17c965eaa62136322f0dec6b1f8e},
abstract = {Various industries such as finance, meteorology, and energy generate vast amounts of heterogeneous data every day. There is a natural demand for humans to manage, process, and display data efficiently. However, it necessitates labor-intensive efforts and a high level of expertise for these data-related tasks. Considering that large language models (LLMs) have showcased promising capabilities in semantic understanding and reasoning, we advocate that the deployment of LLMs could autonomously manage and process massive amounts of data while displaying and interacting in a human-friendly manner. Based on this belief, we propose Data-Copilot, an LLM-based system that connects numerous data sources on one end and caters to diverse human demands on the other end. Acting like an experienced expert, Data-Copilot autonomously transforms raw data into visualization results that best match the user's intent. Specifically, Data-Copilot autonomously designs versatile interfaces (tools) for data management, processing, prediction, and visualization. In real-time response, it automatically deploys a concise workflow by invoking corresponding interfaces step by step for the user's request. The interface design and deployment processes are fully controlled by Data-Copilot itself, without human assistance. Besides, we create a Data-Copilot demo that links abundant data from different domains (stock, fund, company, economics, and live news) and accurately respond to diverse requests, serving as a reliable AI assistant.},
author = {Wenqi Zhang and Yongliang Shen and Weiming Lu and Y. Zhuang},
journal = {ArXiv},
volume = {abs/2306.07209},
pages = {null},
doi = {10.48550/arXiv.2306.07209},
arxivid = {2306.07209},
}

@article{d6811adf94d85108b30da82a851ad364823fb9db,
title = {Text2Motion: From Natural Language Instructions to Feasible Plans},
year = {2023},
url = {https://www.semanticscholar.org/paper/d6811adf94d85108b30da82a851ad364823fb9db},
abstract = {We propose Text2Motion, a language-based planning framework enabling robots to solve sequential manipulation tasks that require long-horizon reasoning. Given a natural language instruction, our framework constructs both a task- and motion-level plan that is verified to reach inferred symbolic goals. Text2Motion uses feasibility heuristics encoded in Q-functions of a library of skills to guide task planning with Large Language Models. Whereas previous language-based planners only consider the feasibility of individual skills, Text2Motion actively resolves geometric dependencies spanning skill sequences by performing geometric feasibility planning during its search. We evaluate our method on a suite of problems that require long-horizon reasoning, interpretation of abstract goals, and handling of partial affordance perception. Our experiments show that Text2Motion can solve these challenging problems with a success rate of 82%, while prior state-of-the-art language-based planning methods only achieve 13%. Text2Motion thus provides promising generalization characteristics to semantically diverse sequential manipulation tasks with geometric dependencies between skills.},
author = {Kevin Lin and Christopher Agia and Toki Migimatsu and M. Pavone and J. Bohg},
journal = {ArXiv},
volume = {abs/2303.12153},
pages = {null},
doi = {10.48550/arXiv.2303.12153},
arxivid = {2303.12153},
}

@article{574beee702be3856d60aa482ec725168fe64fc99,
title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
year = {2023},
url = {https://www.semanticscholar.org/paper/574beee702be3856d60aa482ec725168fe64fc99},
abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
author = {Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and John A. Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Y. Lee and Yuan-Fang Li and Scott M. Lundberg and Harsha Nori and H. Palangi and Marco Tulio Ribeiro and Yi Zhang},
journal = {ArXiv},
volume = {abs/2303.12712},
pages = {null},
doi = {10.48550/arXiv.2303.12712},
arxivid = {2303.12712},
}

@article{41531594d7e0f3b2e138ae43e0a0f6e24a9b014c,
title = {Code as Policies: Language Model Programs for Embodied Control},
year = {2022},
url = {https://www.semanticscholar.org/paper/41531594d7e0f3b2e138ae43e0a0f6e24a9b014c},
abstract = {Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
author = {Jacky Liang and Wenlong Huang and F. Xia and Peng Xu and Karol Hausman and Brian Ichter and Peter R. Florence and Andy Zeng},
journal = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
volume = {null},
pages = {9493-9500},
doi = {10.1109/ICRA48891.2023.10160591},
arxivid = {2209.07753},
}

@article{c03fa01fbb9c77fe3d10609ba5f1dee33a723867,
title = {ProgPrompt: Generating Situated Robot Task Plans using Large Language Models},
year = {2022},
url = {https://www.semanticscholar.org/paper/c03fa01fbb9c77fe3d10609ba5f1dee33a723867},
abstract = {Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io},
author = {Ishika Singh and Valts Blukis and Arsalan Mousavian and Ankit Goyal and Danfei Xu and Jonathan Tremblay and D. Fox and Jesse Thomason and Animesh Garg},
journal = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
volume = {null},
pages = {11523-11530},
doi = {10.1109/ICRA48891.2023.10161317},
arxivid = {2209.11302},
}

@article{570079bbdd8758dfe865097e05719313c9c1301a,
title = {LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model},
year = {2023},
url = {https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a},
abstract = {How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augment LLaMA-Adapter by unlocking more learnable parameters (e.g., norm, bias and scale), which distribute the instruction-following ability across the entire LLaMA model besides adapters. Secondly, we propose an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation. Thirdly, a joint training paradigm of image-text pairs and instruction-following data is introduced by optimizing disjoint groups of learnable parameters. This strategy effectively alleviates the interference between the two tasks of image-text alignment and instruction following and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset. During inference, we incorporate additional expert models (e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image understanding capability without incurring training costs. Compared to the original LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal instructions by merely introducing 14M parameters over LLaMA. The newly designed framework also exhibits stronger language-only instruction-following capabilities and even excels in chat interactions. Our code and models are available at https://github.com/ZrrSkywalker/LLaMA-Adapter.},
author = {Peng Gao and Jiaming Han and Renrui Zhang and Ziyi Lin and Shijie Geng and Aojun Zhou and W. Zhang and Pan Lu and Conghui He and Xiangyu Yue and Hongsheng Li and Y. Qiao},
journal = {ArXiv},
volume = {abs/2304.15010},
pages = {null},
doi = {10.48550/arXiv.2304.15010},
arxivid = {2304.15010},
}

@article{af997821231898a5f8d0fd78dad4eec526acabe5,
title = {Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models},
year = {2023},
url = {https://www.semanticscholar.org/paper/af997821231898a5f8d0fd78dad4eec526acabe5},
abstract = {ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \url{https://github.com/microsoft/visual-chatgpt}.},
author = {Chenfei Wu and Sheng-Kai Yin and Weizhen Qi and Xiaodong Wang and Zecheng Tang and Nan Duan},
journal = {ArXiv},
volume = {abs/2303.04671},
pages = {null},
doi = {10.48550/arXiv.2303.04671},
arxivid = {2303.04671},
}

@article{92a8f7f09f3705cb5a6009a42220a6f01ea084e8,
title = {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
year = {2022},
url = {https://www.semanticscholar.org/paper/92a8f7f09f3705cb5a6009a42220a6f01ea084e8},
abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g."make breakfast"), to a chosen set of actionable steps (e.g."open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner},
author = {Wenlong Huang and P. Abbeel and Deepak Pathak and Igor Mordatch},
journal = {ArXiv},
volume = {abs/2201.07207},
pages = {null},
arxivid = {2201.07207},
}

@article{3f5b31c4f7350dc88002c121aecbdc82f86eb5bb,
title = {BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
year = {2023},
url = {https://www.semanticscholar.org/paper/3f5b31c4f7350dc88002c121aecbdc82f86eb5bb},
abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
author = {Junnan Li and Dongxu Li and S. Savarese and Steven Hoi},
journal = {ArXiv},
volume = {abs/2301.12597},
pages = {null},
doi = {10.48550/arXiv.2301.12597},
arxivid = {2301.12597},
}

@article{6e754273d54a91371efbc928cd6b156364d517da,
title = {ViperGPT: Visual Inference via Python Execution for Reasoning},
year = {2023},
url = {https://www.semanticscholar.org/paper/6e754273d54a91371efbc928cd6b156364d517da},
abstract = {Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, limiting interpretability and generalization. Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously. We introduce ViperGPT, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. ViperGPT utilizes a provided API to access the available modules, and composes them by generating Python code that is later executed. This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks.},
author = {D'idac Sur'is and Sachit Menon and Carl Vondrick},
journal = {ArXiv},
volume = {abs/2303.08128},
pages = {null},
doi = {10.48550/arXiv.2303.08128},
arxivid = {2303.08128},
}

@article{f44ad7ad67ddd5fe74598fe491ca75c5221380df,
title = {MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
year = {2023},
url = {https://www.semanticscholar.org/paper/f44ad7ad67ddd5fe74598fe491ca75c5221380df},
abstract = {The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. We believe the primary reason for GPT-4's advanced multi-modal generation capabilities lies in the utilization of a more advanced large language model (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one projection layer. Our findings reveal that MiniGPT-4 possesses many capabilities similar to those exhibited by GPT-4 like detailed image description generation and website creation from hand-written drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, providing solutions to problems shown in images, teaching users how to cook based on food photos, etc. In our experiment, we found that only performing the pretraining on raw image-text pairs could produce unnatural language outputs that lack coherency including repetition and fragmented sentences. To address this problem, we curate a high-quality, well-aligned dataset in the second stage to finetune our model using a conversational template. This step proved crucial for augmenting the model's generation reliability and overall usability. Notably, our model is highly computationally efficient, as we only train a projection layer utilizing approximately 5 million aligned image-text pairs. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.},
author = {Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
journal = {ArXiv},
volume = {abs/2304.10592},
pages = {null},
doi = {10.48550/arXiv.2304.10592},
arxivid = {2304.10592},
}

@article{9dee1aceb09f7d4c22fdbaf49d238e1502effd1b,
title = {Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought},
year = {2023},
url = {https://www.semanticscholar.org/paper/9dee1aceb09f7d4c22fdbaf49d238e1502effd1b},
abstract = {Language instructions and demonstrations are two natural ways for users to teach robots personalized tasks. Recent progress in Large Language Models (LLMs) has shown impressive performance in translating language instructions into code for robotic tasks. However, translating demonstrations into task code continues to be a challenge due to the length and complexity of both demonstrations and code, making learning a direct mapping intractable. This paper presents Demo2Code, a novel framework that generates robot task code from demonstrations via an extended chain-of-thought and defines a common latent specification to connect the two. Our framework employs a robust two-stage process: (1) a recursive summarization technique that condenses demonstrations into concise specifications, and (2) a code synthesis approach that expands each function recursively from the generated specifications. We conduct extensive evaluation on various robot task benchmarks, including a novel game benchmark Robotouille, designed to simulate diverse cooking tasks in a kitchen environment. The project's website is available at https://portal-cornell.github.io/demo2code-webpage},
author = {Huaxiaoyue Wang and Gonzalo Gonzalez-Pumariega and Yash Sharma and Sanjiban Choudhury},
journal = {ArXiv},
volume = {abs/2305.16744},
pages = {null},
doi = {10.48550/arXiv.2305.16744},
arxivid = {2305.16744},
}

@article{c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4,
title = {MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action},
year = {2023},
url = {https://www.semanticscholar.org/paper/c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4},
abstract = {We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos. MM-REACT's prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore, we discuss and compare MM-REACT's system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning. Code, demo, video, and visualization are available at https://multimodal-react.github.io/},
author = {Zhengyuan Yang and Linjie Li and Jianfeng Wang and Kevin Lin and E. Azarnasab and Faisal Ahmed and Zicheng Liu and Ce Liu and Michael Zeng and Lijuan Wang},
journal = {ArXiv},
volume = {abs/2303.11381},
pages = {null},
doi = {10.48550/arXiv.2303.11381},
arxivid = {2303.11381},
}

@article{d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43,
title = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face},
year = {2023},
url = {https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43},
abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards artificial general intelligence.},
author = {Yongliang Shen and Kaitao Song and Xu Tan and D. Li and Weiming Lu and Y. Zhuang},
journal = {ArXiv},
volume = {abs/2303.17580},
pages = {null},
doi = {10.48550/arXiv.2303.17580},
arxivid = {2303.17580},
}

@article{9a75e23639bfcc3a51da57a3b682a984d1d8ac0b,
title = {Language Models can Solve Computer Tasks},
year = {2023},
url = {https://www.semanticscholar.org/paper/9a75e23639bfcc3a51da57a3b682a984d1d8ac0b},
abstract = {Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.},
author = {Geunwoo Kim and P. Baldi and S. McAleer},
journal = {ArXiv},
volume = {abs/2303.17491},
pages = {null},
doi = {10.48550/arXiv.2303.17491},
arxivid = {2303.17491},
}

@article{7ca954844bc1dd405bc43445b1c990e42d865095,
title = {CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society},
year = {2023},
url = {https://www.semanticscholar.org/paper/7ca954844bc1dd405bc43445b1c990e42d865095},
abstract = {The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their"cognitive"processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond. The GitHub repository of this project is made publicly available on: https://github.com/lightaime/camel.},
author = {G. Li and H. Hammoud and Hani Itani and Dmitrii Khizbullin and Bernard Ghanem},
journal = {ArXiv},
volume = {abs/2303.17760},
pages = {null},
doi = {10.48550/arXiv.2303.17760},
arxivid = {2303.17760},
}

@article{3aaf6a2cbad5850ad81ab5c163599cb3d523436f,
title = {Self-Refine: Iterative Refinement with Self-Feedback},
year = {2023},
url = {https://www.semanticscholar.org/paper/3aaf6a2cbad5850ad81ab5c163599cb3d523436f},
abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
author = {Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and S. Welleck and Bodhisattwa Prasad Majumder and Shashank Gupta and A. Yazdanbakhsh and Peter Clark},
journal = {ArXiv},
volume = {abs/2303.17651},
pages = {null},
doi = {10.48550/arXiv.2303.17651},
arxivid = {2303.17651},
}

@article{cb5e3f085caefd1f3d5e08637ab55d39e61234fc,
title = {Do As I Can, Not As I Say: Grounding Language in Robotic Affordances},
year = {2022},
url = {https://www.semanticscholar.org/paper/cb5e3f085caefd1f3d5e08637ab55d39e61234fc},
abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's"hands and eyes,"while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.},
author = {Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and K. Gopalakrishnan and Karol Hausman and Alexander Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and A. Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and N. Joshi and Ryan C. Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and S. Levine and Yao Lu and Linda Luu and Carolina Parada and P. Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and D. Reyes and P. Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and F. Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan},
arxivid = {2204.01691},
}

@article{ac7771c332da42b29a913b116bd6ef622cbf89cf,
title = {TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs},
year = {2023},
url = {https://www.semanticscholar.org/paper/ac7771c332da42b29a913b116bd6ef622cbf89cf},
abstract = {Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation models to propose task solution outlines and then automatically match some of the sub-tasks in the outlines to the off-the-shelf models and systems with special functionalities to complete them. Inspired by this, we introduce TaskMatrix.AI as a new AI ecosystem that connects foundation models with millions of APIs for task completion. Unlike most previous work that aimed to improve a single AI model, TaskMatrix.AI focuses more on using existing foundation models (as a brain-like central system) and APIs of other AI models and systems (as sub-task solvers) to achieve diversified tasks in both digital and physical domains. As a position paper, we will present our vision of how to build such an ecosystem, explain each key component, and use study cases to illustrate both the feasibility of this vision and the main challenges we need to address next.},
author = {Yaobo Liang and Chenfei Wu and Ting Song and Wenshan Wu and Yan Xia and Yu Liu and Yangyiwen Ou and Shuai Lu and Lei Ji and Shaoguang Mao and Yun Wang and Linjun Shou and Ming Gong and Nan Duan},
journal = {ArXiv},
volume = {abs/2303.16434},
pages = {null},
doi = {10.48550/arXiv.2303.16434},
arxivid = {2303.16434},
}

@article{f197bf0fc2f228483f6af3285000d54d8d97f9eb,
title = {Voyager: An Open-Ended Embodied Agent with Large Language Models},
year = {2023},
url = {https://www.semanticscholar.org/paper/f197bf0fc2f228483f6af3285000d54d8d97f9eb},
abstract = {We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.},
author = {Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi (Jim) Fan and Anima Anandkumar},
journal = {ArXiv},
volume = {abs/2305.16291},
pages = {null},
doi = {10.48550/arXiv.2305.16291},
arxivid = {2305.16291},
}

@article{2d2ca2e54c54748557b8aac7d328ce32ebfe8944,
title = {ReAct: Synergizing Reasoning and Acting in Language Models},
year = {2022},
url = {https://www.semanticscholar.org/paper/2d2ca2e54c54748557b8aac7d328ce32ebfe8944},
abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
author = {Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and I. Shafran and Karthik Narasimhan and Yuan Cao},
journal = {ArXiv},
volume = {abs/2210.03629},
pages = {null},
doi = {10.48550/arXiv.2210.03629},
arxivid = {2210.03629},
}

@article{53d128ea815bcc0526856eb5a9c42cc977cb36a7,
title = {Toolformer: Language Models Can Teach Themselves to Use Tools},
year = {2023},
url = {https://www.semanticscholar.org/paper/53d128ea815bcc0526856eb5a9c42cc977cb36a7},
abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
author = {Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and M. Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
journal = {ArXiv},
volume = {abs/2302.04761},
pages = {null},
doi = {10.48550/arXiv.2302.04761},
arxivid = {2302.04761},
}

@article{154493f69d7db3d49da0e51df0192c6ad5f1724a,
title = {Larger language models do in-context learning differently},
year = {2023},
url = {https://www.semanticscholar.org/paper/154493f69d7db3d49da0e51df0192c6ad5f1724a},
abstract = {We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.},
author = {Jerry W. Wei and Jason Wei and Yi Tay and Dustin Tran and Albert Webson and Yifeng Lu and Xinyun Chen and Hanxiao Liu and Da Huang and Denny Zhou and Tengyu Ma},
journal = {ArXiv},
volume = {abs/2303.03846},
pages = {null},
doi = {10.48550/arXiv.2303.03846},
arxivid = {2303.03846},
}

@article{e01515c6138bc525f7aec30fc85f2adf028d4156,
title = {Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision},
year = {2023},
url = {https://www.semanticscholar.org/paper/e01515c6138bc525f7aec30fc85f2adf028d4156},
abstract = {Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.},
author = {Zhiqing Sun and Yikang Shen and Qinhong Zhou and Hongxin Zhang and Zhenfang Chen and David D. Cox and Yiming Yang and Chuang Gan},
journal = {ArXiv},
volume = {abs/2305.03047},
pages = {null},
doi = {10.48550/arXiv.2305.03047},
arxivid = {2305.03047},
}

@article{d6d3604f369bb0415cbe814e43ca3131323b03e2,
title = {Otter: A Multi-Modal Model with In-Context Instruction Tuning},
year = {2023},
url = {https://www.semanticscholar.org/paper/d6d3604f369bb0415cbe814e43ca3131323b03e2},
abstract = {Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1$\times$ A100 GPU to 4$\times$ RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines.},
author = {Bo Li and Yuanhan Zhang and Liangyu Chen and Jinghao Wang and Jingkang Yang and Ziwei Liu},
journal = {ArXiv},
volume = {abs/2305.03726},
pages = {null},
doi = {10.48550/arXiv.2305.03726},
arxivid = {2305.03726},
}

@article{54a8b153ed04a872da878d695239bdc413dc782c,
title = {InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language},
year = {2023},
url = {https://www.semanticscholar.org/paper/54a8b153ed04a872da878d695239bdc413dc782c},
abstract = {We present an interactive visual framework named InternGPT, or iGPT for short. The framework integrates chatbots that have planning and reasoning capabilities, such as ChatGPT, with non-verbal instructions like pointing movements that enable users to directly manipulate images or videos on the screen. Pointing (including gestures, cursors, etc.) movements can provide more flexibility and precision in performing vision-centric tasks that require fine-grained control, editing, and generation of visual content. The name InternGPT stands for \textbf{inter}action, \textbf{n}onverbal, and \textbf{chat}bots. Different from existing interactive systems that rely on pure language, by incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2. Additionally, in iGPT, an auxiliary control mechanism is used to improve the control capability of LLM, and a large vision-language model termed Husky is fine-tuned for high-quality multi-modal dialogue (impressing ChatGPT-3.5-turbo with 93.89\% GPT-4 Quality). We hope this work can spark new ideas and directions for future interactive visual systems. Welcome to watch the code at https://github.com/OpenGVLab/InternGPT.},
author = {Zhaoyang Liu and Yinan He and Wenhai Wang and Weiyun Wang and Yi Wang and Shoufa Chen and Qing-Long Zhang and Yang Yang and Qingyun Li and Jiashuo Yu and Kunchang Li and Zhe Chen and Xuecheng Yang and Xizhou Zhu and Yali Wang and Limin Wang and Ping Luo and Jifeng Dai and Yu Qiao},
journal = {ArXiv},
volume = {abs/2305.05662},
pages = {null},
doi = {10.48550/arXiv.2305.05662},
arxivid = {2305.05662},
}

@article{7e32aac43e9f1df49e116add03327ee6f365dbf3,
title = {mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality},
year = {2023},
url = {https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3},
abstract = {Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.},
author = {Qinghao Ye and Haiyang Xu and Guohai Xu and Jiabo Ye and Ming Yan and Yi Zhou and Junyan Wang and Anwen Hu and Pengcheng Shi and Yaya Shi and Chenliang Li and Yuanhong Xu and Hehong Chen and Junfeng Tian and Qiang Qi and Ji Zhang and Feiyan Huang},
journal = {ArXiv},
volume = {abs/2304.14178},
pages = {null},
doi = {10.48550/arXiv.2304.14178},
arxivid = {2304.14178},
}

@article{8bd6a2a89503be083176f2cc26fabedb79238cbd,
title = {InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
year = {2023},
url = {https://www.semanticscholar.org/paper/8bd6a2a89503be083176f2cc26fabedb79238cbd},
abstract = {Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.},
author = {Wenliang Dai and Junnan Li and Dongxu Li and A. M. H. Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
journal = {ArXiv},
volume = {abs/2305.06500},
pages = {null},
doi = {10.48550/arXiv.2305.06500},
arxivid = {2305.06500},
}

@article{d48cb91b9e555194f7494c4d4bb9815021d3ee45,
title = {VideoChat: Chat-Centric Video Understanding},
year = {2023},
url = {https://www.semanticscholar.org/paper/d48cb91b9e555194f7494c4d4bb9815021d3ee45},
abstract = {In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems. Preliminary qualitative experiments reveal our system's potential across a broad spectrum of video applications and set the standard for future research. Access our code and data at https://github.com/OpenGVLab/Ask-Anything},
author = {Kunchang Li and Yinan He and Yi Wang and Yizhuo Li and Wen Wang and Ping Luo and Yali Wang and Limin Wang and Yu Qiao},
journal = {ArXiv},
volume = {abs/2305.06355},
pages = {null},
doi = {10.48550/arXiv.2305.06355},
arxivid = {2305.06355},
}

@article{170c97c7215f42edfb20c2248f954879e91ef86e,
title = {Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models},
year = {2023},
url = {https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e},
abstract = {Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner.},
author = {Pan Lu and Baolin Peng and Hao Cheng and Michel Galley and Kai-Wei Chang and Y. Wu and Song-Chun Zhu and Jianfeng Gao},
journal = {ArXiv},
volume = {abs/2304.09842},
pages = {null},
doi = {10.48550/arXiv.2304.09842},
arxivid = {2304.09842},
}

@article{e8cc6a50bc9c6a1e1ed7fdde0e7ccbb4efd4b505,
title = {AVIS: Autonomous Visual Information Seeking with Large Language Models},
year = {2023},
url = {https://www.semanticscholar.org/paper/e8cc6a50bc9c6a1e1ed7fdde0e7ccbb4efd4b505},
abstract = {In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as"What event is commemorated by the building depicted in this image?", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information from the tool outputs, and a working memory component that retains the acquired information throughout the process. The collected user behavior serves as a guide for our system in two key ways. First, we create a transition graph by analyzing the sequence of decisions made by users. This graph delineates distinct states and confines the set of actions available at each state. Second, we use examples of user decision-making to provide our LLM-powered planner and reasoner with relevant contextual instances, enhancing their capacity to make informed decisions. We show that AVIS achieves state-of-the-art results on knowledge-intensive visual question answering benchmarks such as Infoseek and OK-VQA.},
author = {Ziniu Hu and Ahmet Iscen and Chen Sun and Kai-Wei Chang and Yizhou Sun and David A. Ross and C. Schmid and A. Fathi},
journal = {ArXiv},
volume = {abs/2306.08129},
pages = {null},
doi = {10.48550/arXiv.2306.08129},
arxivid = {2306.08129},
}

@article{6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f,
title = {Album Storytelling with Iterative Story-aware Captioning and Large Language Models},
year = {2023},
url = {https://www.semanticscholar.org/paper/6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f},
abstract = {This work studies how to transform an album to vivid and coherent stories, a task we refer to as"album storytelling". While this task can help preserve memories and facilitate experience sharing, it remains an underexplored area in current literature. With recent advances in Large Language Models (LLMs), it is now possible to generate lengthy, coherent text, opening up the opportunity to develop an AI assistant for album storytelling. One natural approach is to use caption models to describe each photo in the album, and then use LLMs to summarize and rewrite the generated captions into an engaging story. However, we find this often results in stories containing hallucinated information that contradicts the images, as each generated caption ("story-agnostic") is not always about the description related to the whole story or miss some necessary information. To address these limitations, we propose a new iterative album storytelling pipeline. Specifically, we start with an initial story and build a story-aware caption model to refine the captions using the whole story as guidance. The polished captions are then fed into the LLMs to generate a new refined story. This process is repeated iteratively until the story contains minimal factual errors while maintaining coherence. To evaluate our proposed pipeline, we introduce a new dataset of image collections from vlogs and a set of systematic evaluation metrics. Our results demonstrate that our method effectively generates more accurate and engaging stories for albums, with enhanced coherence and vividness.},
author = {Munan Ning and Yujia Xie and Dongdong Chen and Zeyin Song and Lu Yuan and Yonghong Tian and Qixiang Ye and Liuliang Yuan},
journal = {ArXiv},
volume = {abs/2305.12943},
pages = {null},
doi = {10.48550/arXiv.2305.12943},
arxivid = {2305.12943},
}

@article{8f84dcbad8cd3b5b4d9229c56bc95f24be859a35,
title = {Grounding Language with Visual Affordances over Unstructured Data},
year = {2022},
url = {https://www.semanticscholar.org/paper/8f84dcbad8cd3b5b4d9229c56bc95f24be859a35},
abstract = {Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help correcting the current policies. In this work, we propose a novel approach to efficiently learn general-purpose language-conditioned robot skills from unstructured, offline and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1% of the total data with language. We evaluate our method in extensive experiments both in simulated and real-world robotic tasks, achieving state-of-the-art performance on the challenging CALVIN benchmark and learning over 25 distinct visuomotor manipulation tasks with a single policy in the real world. We find that when paired with LLMs to break down abstract natural language instructions into subgoals via few-shot prompting, our method is capable of completing long-horizon, multi-tier tasks in the real world, while requiring an order of magnitude less data than previous approaches. Code and videos are available at http://hulc2.cs.uni-freiburg.de.},
author = {Oier Mees and Jessica Borja-Diaz and W. Burgard},
journal = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
volume = {null},
pages = {11576-11582},
doi = {10.1109/ICRA48891.2023.10160396},
arxivid = {2210.01911},
}
